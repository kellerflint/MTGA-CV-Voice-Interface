{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Exploration\n",
    "\n",
    "I tried several different approaches for handling real-time object detection. Though YOLO was the obvious first choice, I wanted to first see if there were other models that could deliver similar performance without fine tuning. If game objects could be recognized without fine tuning, it would open the door for a far more generalizable voice to action interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO\n",
    "\n",
    "The full implementation and testing of the YOLO model can be found in the model_train.ipynb file. \n",
    "\n",
    "Here I just wanted to mention my initial findings when I was first exploring options. Without fine tuning, the YOLO model was incapable of recognizing cards. However it only took a a handful of annotated screenshots to start seeing very high reliability in it's predictions. In fact, it was so good I ended up expanding the classes so that it didn't just detect cards, but also what state the card was in (for instance if a card is tapped or sick). This is important because in magic it is often the case that multiple cards will share the same name. If our player is trying to interact with a card, it is helpful to understand which specific instance of that card they might be referring to. This state information can be helpful in determining this. For instance if they are trying to attack with a card, it is probably not a sick or tapped instance of it, since those cards cannot be used until the next turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grounding Dino\n",
    "\n",
    "One of the first option I came across was grounding Dino. It is capable of delivering real-time performance without having to be fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from mss import mss\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import groundingdino.datasets.transforms as T\n",
    "import torch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroundingDINO Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"GroundingDINO/weights/groundingdino_swint_ogc.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"../mtga_data/z_screen_16.png\"\n",
    "#IMAGE_PATH = \"shot.png\"\n",
    "\n",
    "TEXT_PROMPT = \"card\"\n",
    "BOX_TRESHOLD = 0.25\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "image_source, image = load_image(IMAGE_PATH)\n",
    "\n",
    "print(type(image))\n",
    "print(image.shape)\n",
    "print(type(image_source))\n",
    "\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_TRESHOLD,\n",
    "    text_threshold=TEXT_TRESHOLD\n",
    ")\n",
    "\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "cv2.imwrite(\"annotated_image2.jpg\", annotated_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time GroundingDINO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model\n",
    "\n",
    "model = load_model(\"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"GroundingDINO/weights/groundingdino_swint_ogc.pth\")\n",
    "IMAGE_PATH = \"../yolo_tune/mtga_data/z_screen_5.png\"\n",
    "TEXT_PROMPT = \"orange button\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "def load_cv2_image(cv2_image):\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    image_source = Image.fromarray(cv2_image)\n",
    "    image = np.asarray(image_source)\n",
    "    image_transformed, _ = transform(image_source, None)\n",
    "    return image, image_transformed\n",
    "\n",
    "\n",
    "monitor = {\"top\": 0, \"left\": 0, \"width\": 1920, \"height\": 1080}\n",
    "sct = mss()\n",
    "\n",
    "def process_frame(frame):\n",
    "    image = cv2.cvtColor(np.array(frame), cv2.COLOR_BGRA2RGB)\n",
    "    image_source, processed_image = load_cv2_image(image)\n",
    "\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=model,\n",
    "        image=processed_image,\n",
    "        caption=TEXT_PROMPT,\n",
    "        box_threshold=BOX_TRESHOLD,\n",
    "        text_threshold=TEXT_TRESHOLD\n",
    "    )\n",
    "\n",
    "    annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "\n",
    "    return annotated_frame\n",
    "\n",
    "def main():\n",
    "    cv2.namedWindow(\"Live Labels\", cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(\"Live Labels\", 960, 540)\n",
    "\n",
    "    while True:\n",
    "        screen = sct.grab(monitor)\n",
    "        \n",
    "        labeled_frame = process_frame(screen)\n",
    "\n",
    "        cv2.imshow(\"Live Labels\", labeled_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GroundingDINO Output](images/GroundingDINO_output.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroundingDINO Conclusion\n",
    "\n",
    "GroundingDINO is reasonably good at zero shot object identification. However it is much less consistent than a tuned model. As can been seen in the image above, it is not very confident in it's predictions and often cuts out the mana cost of cards as they tend to float slightly above the cards (at the top right). It is also significantly slower than YOLO and struggles to handle multi-class detection. For instance, I would like both UI elements and cards to be detected, but GroundingDINO seems to struggle as the number of classes it is asked to identify increases and it usually fails to consistently identify either classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment Anything\n",
    "\n",
    "I also tried segmentation with a SAM model. I though that image segmentation might let me detect objects and then a future step could classify or perform OCR to identify the discrete objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_checkpoint = \"sam_vit_b_01ec64.pth\" # Used the lightest model for speed\n",
    "model_type = \"vit_b\"  # others options were vit_b, vit_l, vit_h\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_generator = SamAutomaticMaskGenerator(\n",
    "    sam,\n",
    "    points_per_side=16,\n",
    "    pred_iou_thresh=0.8,\n",
    "    stability_score_thresh=0.95,\n",
    "    min_mask_region_area=100\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = \"z_screen_13.png\"  # Replace with your image path\n",
    "image = cv2.imread(image_path)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for SAM\n",
    "\n",
    "# Resize for faster processing\n",
    "image_resized = cv2.resize(image_rgb, (0, 0), fx=0.5, fy=0.5)\n",
    "\n",
    "print(f\"Image shape: {image_resized.shape}\")\n",
    "\n",
    "# Generate masks\n",
    "masks = mask_generator.generate(image_resized)\n",
    "\n",
    "print(f\"Number of masks: {len(masks)}\")\n",
    "\n",
    "# Visualize and save the segmentation results\n",
    "def overlay_masks(image, masks):\n",
    "    \"\"\"Overlay masks on the image with random colors.\"\"\"\n",
    "    overlay = image.copy()\n",
    "    for mask in masks:\n",
    "        segmentation = mask[\"segmentation\"]\n",
    "        color = np.random.randint(0, 255, size=(3,), dtype=np.uint8)\n",
    "        overlay[segmentation] = 0.5 * overlay[segmentation] + 0.5 * color\n",
    "        contours, _ = cv2.findContours(segmentation.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cv2.drawContours(overlay, contours, -1, color.tolist(), thickness=2)\n",
    "    return overlay\n",
    "\n",
    "# Apply the overlay\n",
    "segmented_image = overlay_masks(image_resized, masks)\n",
    "\n",
    "# Save and display the result\n",
    "output_path = \"segmented_image.jpg\"\n",
    "cv2.imwrite(output_path, cv2.cvtColor(segmented_image, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SAM Output](images/SAM_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Anything Conclusion\n",
    "\n",
    "This segmentation model was sometimes very accurate in detecting card objects but could not do it consistently. Sometimes it identified shapes within cards rather than the card itself and it often focused on background objects rather than the cards. It also proved far too slow to be used for real-time inference. I still think it would be an interesting option to explore in the future as I believe there are faster segmentation models. I also think a fine-tuned segmentation model could be a more accurate option than YOLO for game object clicking since it finds exact edges rather than bounding boxes which could be helpful for overlapping or rotated cards."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
