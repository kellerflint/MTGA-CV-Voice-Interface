{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Demo\n",
    "\n",
    "Below is a demo of the application. It works as follows:\n",
    "- Listens for user input over audio device\n",
    "- Transcribes audio when it detects the end of voice activity\n",
    "- Takes a screenshot of the main window\n",
    "- Makes use of the tuned YOLO model to detect classes and bounding boxes for cards and buttons\n",
    "- Uses OCR to extract the text on the objects to identify them\n",
    "- Calls an LLM that maps the user command to the extracted card text\n",
    "- LLM returns tool calls to move cursor, click objects, and play cards as directed by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import easyocr\n",
    "import numpy as np\n",
    "from mss import mss\n",
    "import whisper\n",
    "import pyaudio\n",
    "import webrtcvad\n",
    "import wave\n",
    "import os\n",
    "import datetime\n",
    "from groq import Groq\n",
    "import pyautogui\n",
    "import json\n",
    "\n",
    "reader = easyocr.Reader(['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set env variable for API_KEY\n",
    "# %env API_KEY=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices:  9\n",
      "{'index': 0, 'structVersion': 2, 'name': 'Microsoft Sound Mapper - Input', 'hostApi': 0, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "Input Device id  0  -  Microsoft Sound Mapper - Input\n",
      "{'index': 1, 'structVersion': 2, 'name': 'Microphone (USB Camera)', 'hostApi': 0, 'maxInputChannels': 1, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "Input Device id  1  -  Microphone (USB Camera)\n",
      "{'index': 2, 'structVersion': 2, 'name': 'Microphone (Scarlett Solo USB)', 'hostApi': 0, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "Input Device id  2  -  Microphone (Scarlett Solo USB)\n",
      "{'index': 3, 'structVersion': 2, 'name': 'Microphone (NexiGo N660 FHD Web', 'hostApi': 0, 'maxInputChannels': 1, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "Input Device id  3  -  Microphone (NexiGo N660 FHD Web\n",
      "{'index': 4, 'structVersion': 2, 'name': 'Microsoft Sound Mapper - Output', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 5, 'structVersion': 2, 'name': 'Speakers (Scarlett Solo USB)', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 6, 'structVersion': 2, 'name': 'BenQ GW2480 (NVIDIA High Defini', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 7, 'structVersion': 2, 'name': 'BenQ GW2480 (NVIDIA High Defini', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 8, 'structVersion': 2, 'name': 'BenQ GW2480 (NVIDIA High Defini', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n"
     ]
    }
   ],
   "source": [
    "p = pyaudio.PyAudio()\n",
    "info = p.get_host_api_info_by_index(0)\n",
    "numdevices = info.get('deviceCount')\n",
    "\n",
    "print('Devices: ', numdevices)\n",
    "\n",
    "for i in range(0, numdevices):\n",
    "    print(p.get_device_info_by_host_api_device_index(0, i))\n",
    "    if (p.get_device_info_by_host_api_device_index(0, i).get('maxInputChannels')) > 0:\n",
    "        print(\"Input Device id \", i, \" - \", p.get_device_info_by_host_api_device_index(0, i).get('name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDetector:\n",
    "    def __init__(self, audio_out_path, audio_device_index, chunk_size=480, format=pyaudio.paInt16, channels=1, rate=16000, silence_duration=1):\n",
    "        self.audio_out_path = audio_out_path\n",
    "        self.audio_device_index = audio_device_index\n",
    "        self.chunk_size = chunk_size\n",
    "        self.format = format\n",
    "        self.channels = channels\n",
    "        self.rate = rate\n",
    "        self.silence_duration = silence_duration\n",
    "\n",
    "        self.p = pyaudio.PyAudio()\n",
    "        self.vad = webrtcvad.Vad()\n",
    "        self.vad.set_mode(3)  # Set VAD aggressiveness (0-3)\n",
    "\n",
    "    def record_audio(self):\n",
    "        stream = self.p.open(format=self.format, channels=self.channels, rate=self.rate, input=True, frames_per_buffer=self.chunk_size, input_device_index=self.audio_device_index)\n",
    "\n",
    "        print(\"Waiting for speech...\")\n",
    "\n",
    "        frames = []\n",
    "        silence_frames = 0\n",
    "        speech_started = False\n",
    "\n",
    "        while True:\n",
    "            data = stream.read(self.chunk_size)\n",
    "\n",
    "            if not speech_started:\n",
    "                if self.vad.is_speech(data, self.rate):\n",
    "                    speech_started = True\n",
    "                    print(\"Recording started.\")\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            frames.append(data)\n",
    "\n",
    "            if self.vad.is_speech(data, self.rate):\n",
    "                silence_frames = 0\n",
    "            else:\n",
    "                silence_frames += self.chunk_size\n",
    "\n",
    "            if silence_frames >= self.rate * self.silence_duration:\n",
    "                break\n",
    "\n",
    "        print(\"Recording finished at \", datetime.datetime.now())\n",
    "\n",
    "        wf = wave.open(self.audio_out_path, \"wb\")\n",
    "        wf.setnchannels(self.channels)\n",
    "        wf.setsampwidth(self.p.get_sample_size(self.format))\n",
    "        wf.setframerate(self.rate)\n",
    "        wf.writeframes(b\"\".join(frames))\n",
    "        wf.close()\n",
    "\n",
    "        print(f\"Audio saved as {self.audio_out_path}\")\n",
    "\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "\n",
    "    def terminate(self):\n",
    "        self.p.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STT:\n",
    "    def __init__(self):\n",
    "        self.model = whisper.load_model(\"base\")\n",
    "\n",
    "    def transcribe(self, audio_file: str):\n",
    "        print(\"Outputting Audio File\", audio_file)\n",
    "        result = self.model.transcribe(audio_file)\n",
    "        return result[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(\n",
    "    api_key=os.environ[\"API_KEY\"]\n",
    ")\n",
    "\n",
    "def generate_response(messages, tools=[], model=\"llama-3.1-8b-instant\", max_tokens=150, temperature=0.7):\n",
    "    print(\"Generating response for messages:\", messages)\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        tools=tools\n",
    "    )\n",
    "\n",
    "    response = chat_completion.choices[0].message\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool definitions\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"click\",\n",
    "            \"description\": \"Clicks on a card\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"id\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The Card ID to click on.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"goto\",\n",
    "            \"description\": \"Moves the mouse to the location of a card\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"id\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The Card ID to move the mouse to.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"play\",\n",
    "            \"description\": \"Plays a card\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"id\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The Card ID of the card to play.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"id\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool functions\n",
    "def click_tool(id: int, boxes):\n",
    "    print(\"Clicking on\", id)\n",
    "    boxOptions = [box for box in boxes if box.id == id]\n",
    "    if len(boxOptions) == 0: return\n",
    "    box = boxOptions[0]\n",
    "\n",
    "    print(f\"Found box, executing on ({box.xc}, {box.yc})\")\n",
    "    pyautogui.moveTo(box.xc, box.yc, duration=0.25)\n",
    "    pyautogui.mouseDown(); pyautogui.mouseUp()\n",
    "\n",
    "def goto_tool(id, boxes):\n",
    "    print(\"Going to\", id)\n",
    "    boxOptions = [box for box in boxes if box.id == id]\n",
    "    if len(boxOptions) == 0: return\n",
    "    box = boxOptions[0]\n",
    "\n",
    "    print(f\"Found box, executing on ({box.xc}, {box.yc})\")\n",
    "    pyautogui.moveTo(box.xc, box.yc, duration=0.25)\n",
    "\n",
    "def play_tool(id, boxes):\n",
    "    print(\"Playing\", id)\n",
    "    boxOptions = [box for box in boxes if box.id == id]\n",
    "    if len(boxOptions) == 0: return\n",
    "    box = boxOptions[0]\n",
    "\n",
    "    print(f\"Found box, executing on ({box.xc}, {box.yc})\")\n",
    "    pyautogui.moveTo(box.xc, box.yc, duration=0.25)\n",
    "    pyautogui.mouseDown(); pyautogui.mouseUp()\n",
    "    pyautogui.moveTo(box.xc, 500, duration=0.25)\n",
    "    pyautogui.mouseDown(); pyautogui.mouseUp()\n",
    "    \n",
    "\n",
    "# Execute selected tool\n",
    "def execute_tool(tool_name, params={}, boxes=[]):\n",
    "\n",
    "    params = json.loads(params)\n",
    "\n",
    "    # Map tool functions\n",
    "    tool_functions = {\n",
    "        \"click\": click_tool,\n",
    "        \"goto\": goto_tool,\n",
    "        \"play\": play_tool\n",
    "    }\n",
    "\n",
    "    if tool_name in tool_functions:\n",
    "        print(f\"Executing tool: {tool_name}\")\n",
    "        print(params)\n",
    "        return tool_functions[tool_name](**params, boxes=boxes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown tool: {tool_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing tool: click\n",
      "{'id': 1}\n",
      "Clicking on 1\n",
      "Executing tool: goto\n",
      "{'id': 2}\n",
      "Going to 2\n",
      "Executing tool: play\n",
      "{'id': 3}\n",
      "Playing 3\n"
     ]
    }
   ],
   "source": [
    "# Tool Checking\n",
    "execute_tool(\"click\", '{\"id\": 1}')\n",
    "execute_tool(\"goto\", '{\"id\": 2}')\n",
    "execute_tool(\"play\", '{\"id\": 3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(box):\n",
    "    plt.figure()\n",
    "    plt.title(f\"Box: {box.class_name} (Confidence: {box.confidence:.2f})\")\n",
    "    plt.imshow(cv2.cvtColor(box.image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Box:\n",
    "    def __init__(self, id: int, image, x1, y1, x2, y2, class_name, confidence):\n",
    "        self.id = id\n",
    "        self.image = image\n",
    "        self.x1, self.y1, self.x2, self.y2 = x1, y1, x2, y2\n",
    "        self.xc, self.yc = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "        self.class_name = class_name\n",
    "        self.text = self.ocr()\n",
    "        self.confidence = confidence\n",
    "\n",
    "    def ocr(self):\n",
    "        gray = cv2.cvtColor(self.image, cv2.COLOR_BGR2GRAY)\n",
    "        results = reader.readtext(gray)\n",
    "        detected_text = ''.join([result[1] for result in results]) \n",
    "        return detected_text\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Box {self.id}: {self.class_name} ({self.confidence:.2f}) \\nCenter: ({self.xc}, {self.yc}) \\nText: {self.text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(model, frame):\n",
    "    img = cv2.cvtColor(np.array(frame), cv2.COLOR_BGRA2BGR)\n",
    "    \n",
    "    # Run inference\n",
    "    results = model.predict(img, conf=0.5, verbose=False)\n",
    "\n",
    "    # Extract bounding boxes\n",
    "    boxes = []\n",
    "    for i, box in enumerate(results[0].boxes):\n",
    "        x_min, y_min, x_max, y_max = map(int, box.xyxy[0].tolist())\n",
    "        class_id = int(box.cls[0])\n",
    "        class_name = model.names[class_id]\n",
    "        confidence = float(box.conf[0])\n",
    "\n",
    "        boxes.append(Box(\n",
    "            id=i,\n",
    "            image=img[y_min:y_max, x_min:x_max],\n",
    "            x1=x_min, \n",
    "            x2=x_max,\n",
    "            y1=y_min,\n",
    "            y2=y_max, \n",
    "            class_name=class_name,\n",
    "            confidence=confidence\n",
    "        ))\n",
    "\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for speech...\n",
      "Recording started.\n",
      "Recording finished at  2024-12-08 16:26:13.879292\n",
      "Audio saved as output.wav\n",
      "Outputting Audio File output.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dev\\MTGA-CV-Voice-Interface\\.venv\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription completed at 2024-12-08 16:26:14.292483 Text:  Thank you.\n",
      "[{'role': 'system', 'content': \"The user will give you a command and a card name. Ignore any instructions that are not commands to play, click, or move to a card.\\n                            Perform only one action at a time. If the user does not provide instructions or their is unrelated to playing cards, don't call any tools.\\n                            If there are multiple cards with the same name, choose the one that makes the most sense (for instance, a user probably isn't trying to play a 'tapped' or 'sick' card).\\n                            Instructions will start with the command and then tell you the card. Call the appropriate tool and give it the ID of the card the user tells you to play. \\n                            The card text can be somewhat garbled, but do your best to match the card the user asks for with the text of the cards.\"}, {'role': 'user', 'content': 'User Command:\\n Thank you.\\n\\n\\nCards:\\n \\n---\\n'}]\n",
      "Generating response for messages: [{'role': 'system', 'content': \"The user will give you a command and a card name. Ignore any instructions that are not commands to play, click, or move to a card.\\n                            Perform only one action at a time. If the user does not provide instructions or their is unrelated to playing cards, don't call any tools.\\n                            If there are multiple cards with the same name, choose the one that makes the most sense (for instance, a user probably isn't trying to play a 'tapped' or 'sick' card).\\n                            Instructions will start with the command and then tell you the card. Call the appropriate tool and give it the ID of the card the user tells you to play. \\n                            The card text can be somewhat garbled, but do your best to match the card the user asks for with the text of the cards.\"}, {'role': 'user', 'content': 'User Command:\\n Thank you.\\n\\n\\nCards:\\n \\n---\\n'}]\n",
      "Executing tool: goto\n",
      "{'id': -1}\n",
      "Going to -1\n",
      "Waiting for speech...\n"
     ]
    }
   ],
   "source": [
    "AUDIO_DEVICE_INDEX = 2\n",
    "\n",
    "stt_engine = STT()\n",
    "speech_detector = SpeechDetector(\"output.wav\", AUDIO_DEVICE_INDEX)\n",
    "\n",
    "tuned_model = \"yolo11s_tuned_50.pt\"\n",
    "\n",
    "def app():\n",
    "    model = YOLO(tuned_model)\n",
    "\n",
    "    monitor = {\"top\": 0, \"left\": 0, \"width\": 1920, \"height\": 1080}\n",
    "    sct = mss()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            speech_detector.record_audio()\n",
    "            transcription = stt_engine.transcribe(\"output.wav\")\n",
    "            print('Transcription completed at', datetime.datetime.now(), 'Text:', transcription)\n",
    "\n",
    "            screen = sct.grab(monitor)\n",
    "            boxes = process_frame(model, screen)\n",
    "\n",
    "            boxes_context = \"\\n---\\n\"\n",
    "            for box in boxes:\n",
    "                boxes_context += f\"Card ID:\\n{box.id}\\nCard State:{box.class_name}\\nCard Text:\\n{box.text}\\n---\\n\"\n",
    "\n",
    "            messages = []\n",
    "            messages.append({\"role\": \"system\", \"content\": \"\"\"The user will give you a command and a card name. Ignore any instructions that are not commands to play, click, or move to a card.\n",
    "                            Perform only one action at a time. If the user does not provide instructions or their is unrelated to playing cards, don't call any tools.\n",
    "                            If there are multiple cards with the same name, choose the one that makes the most sense (for instance, a user probably isn't trying to play a 'tapped' or 'sick' card).\n",
    "                            Instructions will start with the command and then tell you the card. Call the appropriate tool and give it the ID of the card the user tells you to play. \n",
    "                            The card text can be somewhat garbled, but do your best to match the card the user asks for with the text of the cards.\"\"\"})\n",
    "            messages.append({\"role\": \"user\", \"content\": f\"User Command:\\n{transcription}\\n\\n\\nCards:\\n {boxes_context}\"})\n",
    "            print(messages)\n",
    "            response = generate_response(messages, tools=tools, model=\"llama-3.1-8b-instant\", max_tokens=150, temperature=0.7)\n",
    "\n",
    "            if response.tool_calls:\n",
    "                for tool_call in response.tool_calls:\n",
    "                    execute_tool(tool_call.function.name, tool_call.function.arguments, boxes)\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "\n",
    "app()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
